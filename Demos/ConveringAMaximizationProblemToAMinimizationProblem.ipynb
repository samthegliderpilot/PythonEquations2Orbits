{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting a Maximization Problem to a Minimization Problem\n",
    "## Sam Gilbert\n",
    "### Rough Draft v01 - 27 Apr 2022\n",
    "\n",
    "Throughout the literature the standard technique for converting an optimal control problem from a maximization problem to a minimization problem is to multiply the cost function by negative 1.  This advice, although well suited for working with numerical processes, will yield incorrect conditions for a wide range of problems.  In this document, we will analyze the Problem of Mayer and examine the optimality conditions with and without that negative applied and show that applying that negative 1 yields non-optimal results.\n",
    "\n",
    "In creating a method to solve for the minimum for optimal control problems by analytically deriving the optimality conditions before numerically solving them, difficulty was encountered when trying to adapt a maximization problem found in the literature [1, page 234].  The standard advice in much of the literature was that to do such a conversion, you multiply the cost function by negative 1 [2, page 479].  However doing so will result in nonsensical optimality conditions.  Often this will not matter for numerical optimizers as they are driving the optimality conditions (or trivial algebraic rearrangements of them) to 0.  \n",
    "\n",
    "Note that this is not the same thing as actually trying to find a minimum solution.  The problem for a low-thrust orbit raising (LEO to GEO) is identical for a low-thrust orbit lowering (GEO to LEO); all of the stationary conditions will have identical analytical expressions and sufficient conditions will change predictably.  Continuing with the orbit changing examples, the question that is being analyzed is when we want to solve the orbit raising problem, but the only tools we have available will find a minimum of the problem.\n",
    "\n",
    "We start with the Problem of Mayer where we are maximizing the cost:\n",
    "$$ J = \\phi(t_f) $$\n",
    "Subject to:\n",
    "$$ \\dot{x} = f(t, x, u) $$\n",
    "$$ x(t_0) = x_0 $$\n",
    "$$ \\Psi(t_f, x_f) = 0 $$\n",
    "\n",
    "We will make the following assumptions in addition to the ones needed for the Euler-Lagrange theorem to hold:\n",
    "- Using the standard indirect approach to solving the problem (forming the Hamiltonian and solving for expressions with it and the costate scalars), there exists a solution to this maximization problem that satisfies the following necessary and sufficient conditions:\n",
    "$$ \\dot{\\lambda} = -H^*_x $$\n",
    "$$ H_u = 0 $$\n",
    "$$ \\lambda(t_f) = \\frac{\\partial{\\phi}}{\\partial{x_f}} + \\nu^T\\frac{\\partial{\\Psi}}{\\partial{x_f}}$$\n",
    "$$ H_{uu} <= 0 $$\n",
    "- That solution to the maximization exists, can be found, and is unique.  Although not true in the general case, it can be true enough of the time that it is not unreasonable.  Because the solution is assumed to be unique, whatever modifications we make to the problem or to our minimization conditions will have to evaluate to the same solution as the maximization conditions.\n",
    "- The cost is not simply the final time.  Otherwise we would need a different transversality condition.  However similar analysis can be performed with the differential form of the transversality condition.\n",
    "\n",
    "### Multiplying the Cost by -1 \n",
    "\n",
    "Conventional advice has us create a minimization problem by multiplying the cost by -1.  The problem is now restated as:\n",
    "\n",
    "Minimize the cost:\n",
    "$$ J_{min} = -\\phi(t_f) $$\n",
    "Subject to:\n",
    "$$ \\dot{x} = f(t, x, u) $$\n",
    "$$ x(t_0) = x_0 $$\n",
    "$$ \\Psi(t_f, x_f) = 0 $$\n",
    "\n",
    "Indirect methods start by creating the Hamiltonian.\n",
    "\n",
    "$$ H = L_{min} + \\lambda^T f(x, u, t) $$\n",
    "\n",
    "However the problem of Mayer has no path cost, so the Hamiltonian simplifies to:\n",
    "\n",
    "$$ H = \\lambda^T f(x, u, t) $$\n",
    "\n",
    "Notice that this expression will be identical for both the maximization and minimization paths.  Because of this, the first two necessary conditions ($\\dot{\\lambda}$ and $H_u$) are identical.\n",
    "\n",
    "The expression for the boundary conditions need to result in identical $\\lambda(t_f)$, otherwise a different solution will be found violating the assumption of a unique solution.  None of the terminal constraints have been changed, however the multiplier of constants $\\nu$ may be different.  After some rearranging of the boundary condition expression and equating the conditions for the minimum and maximum case:\n",
    "\n",
    "$$ \\lambda_f -\\nu^T\\frac{\\partial{ \\Psi}}{\\partial{x_f}} = \\frac{\\partial{\\phi}}{\\partial{x_f}}$$\n",
    "\n",
    "$$\\lambda_f - \\nu_{max}^T\\frac{\\partial{ \\Psi}}{\\partial{x_f}}= -\\lambda_f +\\nu_{min}^T\\frac{\\partial{ \\Psi}}{\\partial{x_f}}$$\n",
    "\n",
    "$$\\lambda_f = \\frac{\\nu_{max}^T}{2}\\frac{\\partial{ \\Psi}}{\\partial{x_f}} + \\frac{\\nu_{min}^T}{2}\\frac{\\partial{ \\Psi}}{\\partial{x_f}} $$\n",
    "\n",
    "$$\\lambda_f = \\nu_{avg}^T\\frac{\\partial{ \\Psi}}{\\partial{x_f}} $$\n",
    "\n",
    "Substituting this expression back into the expressions for $\\lambda_f$ gives us the pair of equations: \n",
    "\n",
    "$$\\lambda_f = \\nu_{avg}^T\\frac{\\partial{ \\Psi}}{\\partial{x_f}} + \\frac{\\partial{\\phi}}{\\partial{x_f}} $$\n",
    "$$\\lambda_f = \\nu_{avg}^T\\frac{\\partial{ \\Psi}}{\\partial{x_f}} - \\frac{\\partial{\\phi}}{\\partial{x_f}}$$\n",
    "\n",
    "Which can only be true if $\\frac{\\partial{\\phi}}{\\partial{x_f}}$ is 0 which is not true in the general case.  This shows that changing the sign of the cost function leads to the situation where the boundary conditions on the co-state variables cannot be the same.  This implies that the technique of multiplying the cost function by -1 does not flip a maximization problem to a minimization problem.\n",
    "\n",
    "### The Second-Order Sufficient Condition\n",
    "\n",
    "The sufficient condition $H_{uu}$ will need some adjustment.  For a maximum, it must be negative-semi-definite, and for a minimum it must be positive-semi-definite.  However, the Hamiltonian has not changed even if we multiply the cost by -1, so the analytical expression of this for a given problem is identical.  To 'convert' a maximization problem to a minimization problem, we must multiply the maximization condition by negative 1.\n",
    "\n",
    "For a maximum:\n",
    "$$ H_{uu} <= 0 $$\n",
    "\n",
    "Converting that to a minimum:\n",
    "$$ -H_{uu} >= 0 $$\n",
    "\n",
    "### Intuition and Usefulness\n",
    "\n",
    "The first order necessary conditions are ultimately stationary conditions.  As such they ought to hold regardless if we are trying to maximize or minimize the cost.  The fact that the expression for $\\lambda_f$ will change based on the sign change of the cost should have been a warning.\n",
    "\n",
    "As mentioned above, often numerical solvers are solving for conditions that equal 0, and many are focused on the stationary conditions.  As such there are rarely difficulties with multiplying the cost by -1 to change the sense of optimization.  However this information can be useful when analytical expressions specific to a problem are being derived based on the optimality conditions or if comparisons of different solvers is being performed.\n",
    "\n",
    "#### Sources (TODO better)\n",
    "[1] Optimal Control with Aerospace Applications : Longuski, Guzman, Prussing\n",
    "\n",
    "[2] Applied Optimal Control Optimization, Estimation and Control: Bryosn, Ho"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
